# hybrid_search/search.py
from hybrid_search.database import Database
from hybrid_search.embed import Embed
from hybrid_search.utils import singleton, logger, Config
from typing import Dict, List
from collections import defaultdict
import re


@singleton
class SemanticSearch:
    def __init__(self):
        self.db = Database()
        self.embedder = Embed()
        logger.info("‚úÖ SemanticSearch –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def search(self, query: str, n_results: int = None) -> Dict:
        """‚úÖ –£–õ–£–ß–®–ï–ù–ù–´–ô –ø–æ–∏—Å–∫ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            n_results = n_results or Config.RETRIEVAL_TOP_K

            # 1. Dense + Sparse –ø–æ–∏—Å–∫ (–±–µ—Ä—ë–º –±–æ–ª—å—à–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤)
            dense_vector = self.embedder.embed_text(query)
            sparse_vector = self.embedder.embed_sparse(query)

            candidates = self.db.search(
                dense_vector,
                sparse_vector,
                n_results=n_results * 3  # ‚Üê –ë–æ–ª—å—à–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            )

            if not candidates:
                return {'matches': [], 'query': query}

            # 2. ‚úÖ BOOST –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
            candidates = self._boost_by_title(candidates, query)

            # 3. ‚úÖ RERANK –ü–ï–†–ï–î —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º
            reranked = self.embedder.rerank(query, candidates)

            # 4. ‚úÖ –ì–†–£–ü–ü–ò–†–û–í–ö–ê –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
            grouped = self._group_by_document(reranked)

            # 5. ‚úÖ –ü–†–ò–û–†–ò–¢–ï–¢ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —á–∞–Ω–∫–∞–º–∏
            expanded = self._expand_with_priority(grouped, query, dense_vector, sparse_vector)

            # 6. ‚úÖ –§–ò–ù–ê–õ–¨–ù–´–ô –æ—Ç–±–æ—Ä
            final_matches = expanded[:Config.RERANK_TOP_K]

            logger.info(
                f"üìä –ü–æ–∏—Å–∫: {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ ‚Üí "
                f"{len(reranked)} –ø–æ—Å–ª–µ rerank ‚Üí "
                f"{len(grouped)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ‚Üí "
                f"{len(final_matches)} —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"
            )

            return {'matches': final_matches, 'query': query}

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return {'matches': [], 'query': query, 'error': str(e)}

    def _boost_by_title(self, chunks: List[Dict], query: str) -> List[Dict]:
        """‚úÖ –ü–æ–≤—ã—à–∞–µ—Ç score —á–∞–Ω–∫–∞–º –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –≤ title"""
        query_keywords = set(query.lower().split())
        technical_terms = {'–º–æ–¥–µ–ª—å', 'document', 'base', '–∫–ª–∞—Å—Å', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', '—Å–æ–∑–¥–∞–Ω–∏–µ', '—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è'}

        for chunk in chunks:
            title = chunk.get('metadata', {}).get('title', '').lower()

            # ‚úÖ Boost –µ—Å–ª–∏ title —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            title_words = set(title.split())
            overlap = len(query_keywords & title_words)
            tech_overlap = len(technical_terms & title_words)

            if overlap >= 2 or tech_overlap >= 1:
                chunk['score'] = chunk.get('score', 0) * 1.5  # +50%
                chunk['rerank_score'] = chunk.get('rerank_score', chunk.get('score', 0)) * 1.5
                logger.debug(f"üìà Boost –¥–ª—è '{title[:50]}': +50%")

        return sorted(chunks, key=lambda x: x.get('score', 0), reverse=True)

    def _group_by_document(self, chunks: List[Dict]) -> Dict[str, List[Dict]]:
        """‚úÖ –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ –ø–æ document_id (page_id)"""
        grouped = defaultdict(list)
        for chunk in chunks:
            page_id = chunk['id'].rsplit('-', 1)[0]
            grouped[page_id].append(chunk)

        # ‚úÖ –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –±√≥–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —á–∞–Ω–∫–æ–≤ ‚Äî –≤—ã—à–µ
        sorted_docs = sorted(
            grouped.items(),
            key=lambda x: (
                len(x[1]),  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
                max(c.get('rerank_score', c.get('score', 0)) for c in x[1])  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π score
            ),
            reverse=True
        )

        logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(sorted_docs)}")
        for doc_id, doc_chunks in sorted_docs[:5]:
            avg_score = sum(c.get('rerank_score', c.get('score', 0)) for c in doc_chunks) / len(doc_chunks)
            logger.info(f"   ‚Ä¢ {doc_id}: {len(doc_chunks)} —á–∞–Ω–∫–æ–≤ (avg score: {avg_score:.3f})")

        return dict(sorted_docs)

    def _expand_with_priority(
            self,
            grouped: Dict[str, List[Dict]],
            query: str,
            dense_vector: list,
            sparse_vector: dict
    ) -> List[Dict]:
        """‚úÖ –£–ú–ù–û–ï —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        expanded = []
        seen_ids = set()

        # ‚úÖ –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –∏–∑ —Ç–æ–ø-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        for page_id, doc_chunks in list(grouped.items())[:5]:  # –¢–æ–ø-5 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            for chunk in sorted(doc_chunks, key=lambda x: x.get('rerank_score', x.get('score', 0)), reverse=True):
                if chunk['id'] not in seen_ids:
                    expanded.append(chunk)
                    seen_ids.add(chunk['id'])

            # ‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–æ—Å–µ–¥—è–º–∏ –¥–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            max_score = max(c.get('rerank_score', c.get('score', 0)) for c in doc_chunks)
            window = 3 if max_score >= 0.6 else 2 if max_score >= 0.4 else 1

            for chunk in doc_chunks:
                neighbors = self.db.get_neighbors(chunk['id'], window=window)
                for neighbor in neighbors:
                    if neighbor['id'] not in seen_ids:
                        neighbor['score'] = chunk.get('rerank_score',
                                                      chunk.get('score', 0)) * Config.SEARCH_NEIGHBOR_SCORE_MULTIPLIER
                        neighbor['rerank_score'] = neighbor['score']
                        expanded.append(neighbor)
                        seen_ids.add(neighbor['id'])

        # ‚úÖ –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ score
        expanded = sorted(
            expanded,
            key=lambda x: x.get('rerank_score', x.get('score', 0)),
            reverse=True
        )

        logger.info(f"üîó –ü–æ—Å–ª–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è: {len(expanded)} —á–∞–Ω–∫–æ–≤")
        return expanded
