# hybrid_search/utils.py

import os
import json
import logging
import re
import time
from datetime import datetime, timezone
from typing import Optional, Dict, Any, List
from dotenv import load_dotenv
import requests
from bs4 import BeautifulSoup

load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO").upper(),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class Config:
    """–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞"""

    # ===== –ü–æ–∏—Å–∫ =====
    RETRIEVAL_TOP_K: int = int(os.getenv("RETRIEVAL_TOP_K", 20))

    # ===== –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ =====
    RERANK_TOP_K: int = int(os.getenv("RERANK_TOP_K", 5))
    RERANK_MIN_SCORE: float = float(os.getenv("RERANK_MIN_SCORE", 0.3))
    RERANKER_MODEL: str = os.getenv("RERANKER_MODEL", "cross-encoder/ms-marco-MiniLM-L-6-v2")

    # ===== –ü—Ä–æ–º–ø—Ç =====
    MAX_CONTEXT_TOKENS: int = int(os.getenv("MAX_CONTEXT_TOKENS", 2048))
    INCLUDE_SECTION_IN_PROMPT: bool = os.getenv("INCLUDE_SECTION_IN_PROMPT", "true").lower() == "true"

    # ===== –û—Ç–≤–µ—Ç =====
    RESPONSE_FORMAT: str = os.getenv("RESPONSE_FORMAT", "markdown")
    ALWAYS_SHOW_SOURCES: bool = os.getenv("ALWAYS_SHOW_SOURCES", "true").lower() == "true"
    MAX_SOURCE_LINKS: int = int(os.getenv("MAX_SOURCE_LINKS", 3))

    # ===== ChromaDB =====
    CHROMA_DB_PATH: str = os.getenv("CHROMA_DB_PATH", "./chroma_db")
    CHROMA_COLLECTION: str = os.getenv("CHROMA_COLLECTION", "confluence_index")

    # ===== Confluence =====
    CONFLUENCE_URL: str = os.getenv("CONFLUENCE_URL", "").rstrip('/')
    CONFLUENCE_SPACE_NAME: str = os.getenv("CONFLUENCE_SPACE_NAME", "")

    @classmethod
    def log(cls):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        logger.info("üìã RAG Pipeline Config:")
        logger.info(f"   ‚Ä¢ Retrieval: top_k={cls.RETRIEVAL_TOP_K}")
        logger.info(f"   ‚Ä¢ Rerank: top_k={cls.RERANK_TOP_K}, min_score={cls.RERANK_MIN_SCORE}")
        logger.info(f"   ‚Ä¢ Reranker model: {cls.RERANKER_MODEL}")
        logger.info(
            f"   ‚Ä¢ Prompt: max_tokens={cls.MAX_CONTEXT_TOKENS}, include_section={cls.INCLUDE_SECTION_IN_PROMPT}")
        logger.info(f"   ‚Ä¢ Response: format={cls.RESPONSE_FORMAT}, always_sources={cls.ALWAYS_SHOW_SOURCES}")


def load_env_variable(var_name, default=None):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
    value = os.getenv(var_name, default)
    if value is None:
        raise EnvironmentError(f"Missing environment variable: {var_name}")
    return value


def make_request(url: str, auth_token: str, params: dict = None, method: str = 'GET') -> dict:
    """–î–µ–ª–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ Confluence API —Å retry-–ª–æ–≥–∏–∫–æ–π"""
    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json",
        "Authorization": f"Bearer {auth_token}"
    }

    max_retries = 3
    retry_delay = 2

    for attempt in range(max_retries):
        try:
            response = requests.request(
                method=method,
                url=url,
                params=params,
                headers=headers,
                timeout=30,
                verify=True
            )

            logger.debug(f"API [{response.status_code}]: {url.split('?')[0][-60:]}")

            if response.status_code == 401:
                raise ValueError("‚ùå 401: –ù–µ–≤–µ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω –∏–ª–∏ —Ñ–æ—Ä–º–∞—Ç –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏")
            elif response.status_code == 403:
                raise ValueError("‚ùå 403: –ù–µ—Ç –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞")
            elif response.status_code == 404:
                raise ValueError(f"‚ùå 404: –≠–Ω–¥–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {url}")
            elif response.status_code == 429:
                retry_after = int(response.headers.get('Retry-After', retry_delay * (attempt + 1)))
                logger.warning(f"‚ö†Ô∏è  Rate limit, –∂–¥—ë–º {retry_after} —Å–µ–∫...")
                time.sleep(retry_after)
                continue
            elif response.status_code >= 500:
                if attempt < max_retries - 1:
                    logger.warning(f"‚ö†Ô∏è  –°–µ—Ä–≤–µ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ {response.status_code}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 2}")
                    time.sleep(retry_delay * (attempt + 1))
                    continue
                raise ValueError(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ {response.status_code}")
            elif response.status_code >= 400:
                preview = response.text[:300].replace('\n', ' ')
                raise ValueError(f"‚ùå –û—à–∏–±–∫–∞ {response.status_code}: {preview}")

            if not response.text.strip():
                return {}

            return response.json()

        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                logger.warning(f"‚ö†Ô∏è  –¢–∞–π–º–∞—É—Ç, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 2}")
                time.sleep(retry_delay * (attempt + 1))
                continue
            raise ValueError(f"‚ùå –¢–∞–π–º–∞—É—Ç –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
        except requests.exceptions.ConnectionError as e:
            if attempt < max_retries - 1:
                logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 2}")
                time.sleep(retry_delay * (attempt + 1))
                continue
            raise ValueError(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}")
        except json.JSONDecodeError as e:
            preview = response.text[:400].replace('\n', ' ') if 'response' in locals() else "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞"
            raise ValueError(f"‚ùå –ù–µ JSON-–æ—Ç–≤–µ—Ç:\n{preview}\n\n–û—à–∏–±–∫–∞: {e}")

    raise ValueError("‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫")


def initialize_auth():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω –¥–ª—è Bearer-–∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    return load_env_variable("CONFLUENCE_API_KEY")


def html_to_text(html_data: str) -> str:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HTML –≤ —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
    if not html_data:
        return ""

    soup = BeautifulSoup(html_data, 'html.parser')

    for tag in soup(['script', 'style', 'nav', 'header', 'footer']):
        tag.decompose()

    for tag in soup.find_all(['br', 'p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):
        tag.append('\n')

    text = soup.get_text(separator=' ')
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    return ' '.join(lines)


def extract_metadata_from_confluence(page_data: dict, page_id: str, api_url: str) -> Dict[str, Any]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Ç–≤–µ—Ç–∞ Confluence API.
    """
    # ‚úÖ –ó–ê–©–ò–¢–ê: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ page_data ‚Äî dict
    if not isinstance(page_data, dict):
        logger.warning(f"‚ö†Ô∏è  extract_metadata_from_confluence: page_data –∏–º–µ–µ—Ç —Ç–∏–ø {type(page_data)}")
        return {
            'document_id': str(page_id),
            'title': '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è',
            'section': '',
            'chunk_index': 0,
            'total_chunks': 0,
            'url': f"{api_url}/pages/viewpage.action?pageId={page_id}",
            'page_version': '1',
            'last_updated': '',
            'space_key': '',
            'space_name': '',
            'content_type': 'page',
            # ‚úÖ –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º 'tags' –µ—Å–ª–∏ –ø—É—Å—Ç–æ–π
        }

    # ‚úÖ –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö dict
    version_data = page_data.get('version', {})
    if not isinstance(version_data, dict):
        version_data = {}

    space_data = page_data.get('space', {})
    if not isinstance(space_data, dict):
        space_data = {}

    extensions_data = page_data.get('extensions', {})
    if not isinstance(extensions_data, dict):
        extensions_data = {}

    position_data = extensions_data.get('position', {})
    if not isinstance(position_data, dict):
        position_data = {}

    labels_data = page_data.get('labels', {})
    if not isinstance(labels_data, dict):
        labels_data = {}

    # ‚úÖ –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–±–µ–∑ tags –ø–æ–∫–∞)
    metadata = {
        # –ù–∞–≤–∏–≥–∞—Ü–∏—è
        "document_id": str(page_id),
        "title": page_data.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'),
        "section": position_data.get('position', ''),
        "chunk_index": 0,
        "total_chunks": 0,

        # –ê—Ç—Ä–∏–±—É—Ü–∏—è –∏ —Å—Å—ã–ª–∫–∏
        "url": f"{api_url}/pages/viewpage.action?pageId={page_id}",
        "page_version": str(version_data.get('number', 1)),
        "last_updated": version_data.get('when', ''),

        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
        "space_key": space_data.get('key', ''),
        "space_name": space_data.get('name', ''),

        # –¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        "content_type": "page",
    }

    # ‚úÖ –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–≥–∏ –∏–∑ labels ‚Äî –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –Ω–µ –ø—É—Å—Ç—ã–µ
    labels_results = labels_data.get('results', [])
    if isinstance(labels_results, list) and labels_results:
        tags = [
            lbl.get('name', '')
            for lbl in labels_results
            if isinstance(lbl, dict) and lbl.get('name')
        ]
        # ‚úÖ –î–æ–±–∞–≤–ª—è–µ–º tags —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –Ω–µ –ø—É—Å—Ç–æ–π
        if tags:
            metadata['tags'] = tags

    return metadata


def singleton(cls):
    """–ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω—ã–π singleton"""
    instances = {}
    import threading
    lock = threading.Lock()

    def get_instances(*args, **kwargs):
        with lock:
            if cls not in instances:
                instances[cls] = cls(*args, **kwargs)
            return instances[cls]

    return get_instances


def get_redis_client():
    """–°–æ–∑–¥–∞—ë—Ç Redis-–∫–ª–∏–µ–Ω—Ç —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ env"""
    import redis

    return redis.Redis(
        host=load_env_variable("REDIS_HOST", "redis"),
        port=int(load_env_variable("REDIS_PORT", 6379)),
        db=int(load_env_variable("REDIS_DB", 0)),
        decode_responses=True,
        socket_connect_timeout=5,
        socket_timeout=5,
        retry_on_timeout=True,
        health_check_interval=30
    )


def format_datetime(dt: datetime) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç datetime –¥–ª—è Redis"""
    return dt.strftime('%Y-%m-%dT%H:%M:%S.%f%z')


def parse_datetime(dt_str: str) -> Optional[datetime]:
    """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É –≤—Ä–µ–º–µ–Ω–∏ –∏–∑ Redis"""
    if not dt_str:
        return None
    try:
        return datetime.strptime(dt_str, "%Y-%m-%dT%H:%M:%S.%f%z")
    except ValueError:
        try:
            return datetime.strptime(dt_str, "%Y-%m-%dT%H:%M:%S.%f+0000")
        except ValueError:
            logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É: {dt_str}")
            return None


def truncate_text(text: str, max_tokens: int, model_name: str = "gpt2") -> str:
    """
    –û–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ max_tokens —Å —É—á—ë—Ç–æ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±—ã—Å—Ç—Ä—É—é —ç–≤—Ä–∏—Å—Ç–∏–∫—É –µ—Å–ª–∏ tokenizer –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω.
    """
    # –ë—ã—Å—Ç—Ä–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: ~4 —Å–∏–º–≤–æ–ª–∞ ‚âà 1 —Ç–æ–∫–µ–Ω –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π
    if len(text) <= max_tokens * 4:
        return text

    try:
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)
        tokens = tokenizer.encode(text, add_special_tokens=False)
        if len(tokens) <= max_tokens:
            return text
        truncated = tokenizer.decode(tokens[:max_tokens])
        return truncated + "..."
    except Exception:
        # Fallback: –æ–±—Ä–µ–∑–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        return text[:max_tokens * 4] + "..."


def format_markdown_response(text: str, sources: List[Dict[str, str]] = None) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –≤ Markdown —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏.

    Args:
        text: –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM
        sources: –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å title –∏ url

    Returns:
        –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Markdown-–æ—Ç–≤–µ—Ç
    """
    if not sources or not Config.ALWAYS_SHOW_SOURCES:
        return text

    # –§–æ—Ä–º–∏—Ä—É–µ–º –±–ª–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    source_lines = []
    for src in sources[:Config.MAX_SOURCE_LINKS]:
        title = src.get('title', '–î–æ–∫—É–º–µ–Ω—Ç')
        url = src.get('url', '#')
        section = src.get('section', '')

        if section and Config.INCLUDE_SECTION_IN_PROMPT:
            display_title = f"{title} ‚Äî {section}"
        else:
            display_title = title

        source_lines.append(f"‚Ä¢ [{display_title}]({url})")

    if source_lines:
        return f"{text}\n\nüìé **–ò—Å—Ç–æ—á–Ω–∏–∫–∏**:\n" + "\n".join(source_lines)

    return text
