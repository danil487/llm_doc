# hybrid_search/utils.py
import json
import logging
import os
import time
from datetime import datetime
from typing import Optional, Dict, Any, List

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO").upper(),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class Config:
    """‚úÖ –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞"""

    # ===== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ =====
    FORCE_RELOAD: bool = os.getenv("FORCE_RELOAD", "false").lower() == "true"
    SKIP_LOAD: bool = os.getenv("SKIP_LOAD", "false").lower() == "true"
    ENABLE_PERIODIC_SYNC: bool = os.getenv("ENABLE_PERIODIC_SYNC", "true").lower() == "true"

    # ===== ChromaDB =====
    CHROMA_DB_PATH: str = os.getenv("CHROMA_DB_PATH", "/app/data/chroma_db")
    CHROMA_COLLECTION: str = os.getenv("CHROMA_COLLECTION", "confluence_index")

    # ===== Confluence =====
    CONFLUENCE_URL: str = os.getenv("CONFLUENCE_URL", "").rstrip('/')
    CONFLUENCE_API_KEY: str = os.getenv("CONFLUENCE_API_KEY", "")
    CONFLUENCE_SPACE_NAME: str = os.getenv("CONFLUENCE_SPACE_NAME", "")

    # ===== Ollama =====
    OLLAMA_MODEL: str = os.getenv("OLLAMA_MODEL", "llama3.1")
    OLLAMA_HOST: str = os.getenv("OLLAMA_HOST", "http://ollama:11434")

    # ===== Redis =====
    REDIS_HOST: str = os.getenv("REDIS_HOST", "redis")
    REDIS_PORT: int = int(os.getenv("REDIS_PORT", "6379"))
    REDIS_DB: int = int(os.getenv("REDIS_DB", "0"))
    REDIS_TTL_SECONDS: int = int(os.getenv("REDIS_TTL_SECONDS", "3600"))

    # ===== RAG Pipeline =====
    FORCE_CPU: bool = os.getenv("FORCE_CPU", "false").lower() == "true"
    RETRIEVAL_TOP_K: int = int(os.getenv("RETRIEVAL_TOP_K", "20"))
    RERANK_TOP_K: int = int(os.getenv("RERANK_TOP_K", "15"))
    RERANK_MIN_SCORE: float = float(os.getenv("RERANK_MIN_SCORE", "0.3"))
    RERANKER_MODEL: str = os.getenv("RERANKER_MODEL", "cross-encoder/ms-marco-MiniLM-L-6-v2")
    MAX_CONTEXT_TOKENS: int = int(os.getenv("MAX_CONTEXT_TOKENS", "2048"))
    INCLUDE_SECTION_IN_PROMPT: bool = os.getenv("INCLUDE_SECTION_IN_PROMPT", "true").lower() == "true"
    RESPONSE_FORMAT: str = os.getenv("RESPONSE_FORMAT", "markdown")
    ALWAYS_SHOW_SOURCES: bool = os.getenv("ALWAYS_SHOW_SOURCES", "true").lower() == "true"
    MAX_SOURCE_LINKS: int = int(os.getenv("MAX_SOURCE_LINKS", "3"))

    # ===== –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ =====
    SEARCH_NEIGHBOR_WINDOW: int = int(os.getenv("SEARCH_NEIGHBOR_WINDOW", "1"))
    SEARCH_NEIGHBOR_SCORE_MULTIPLIER: float = float(os.getenv("SEARCH_NEIGHBOR_SCORE_MULTIPLIER", "0.8"))

    # ===== Telegram Bot =====
    TELEGRAM_ENABLED: bool = os.getenv("TELEGRAM_ENABLED", "false").lower() == "true"
    TELEGRAM_BOT_TOKEN: str = os.getenv("TELEGRAM_BOT_TOKEN", "")
    TELEGRAM_WEBHOOK_URL: str = os.getenv("TELEGRAM_WEBHOOK_URL", "")
    TELEGRAM_WEBHOOK_PORT: int = int(os.getenv("TELEGRAM_WEBHOOK_PORT", "8443"))

    # ===== –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ =====
    LOG_LEVEL: str = os.getenv("LOG_LEVEL", "INFO")

    @classmethod
    def log(cls):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        logger.info("üìã RAG Pipeline Config:")
        logger.info(
            f"   ‚Ä¢ –ó–∞–≥—Ä—É–∑–∫–∞: force_reload={cls.FORCE_RELOAD}, skip_load={cls.SKIP_LOAD}, sync={cls.ENABLE_PERIODIC_SYNC}")
        logger.info(f"   ‚Ä¢ ChromaDB: {cls.CHROMA_DB_PATH}/{cls.CHROMA_COLLECTION}")
        logger.info(f"   ‚Ä¢ Confluence: {cls.CONFLUENCE_URL}/{cls.CONFLUENCE_SPACE_NAME}")
        logger.info(f"   ‚Ä¢ Ollama: {cls.OLLAMA_MODEL} @ {cls.OLLAMA_HOST}")
        logger.info(f"   ‚Ä¢ Redis: {cls.REDIS_HOST}:{cls.REDIS_PORT}/{cls.REDIS_DB}")
        logger.info(f"   ‚Ä¢ Retrieval: top_k={cls.RETRIEVAL_TOP_K}")
        logger.info(f"   ‚Ä¢ Rerank: top_k={cls.RERANK_TOP_K}, min_score={cls.RERANK_MIN_SCORE}")
        logger.info(f"   ‚Ä¢ Neighbor: window={cls.SEARCH_NEIGHBOR_WINDOW}, mult={cls.SEARCH_NEIGHBOR_SCORE_MULTIPLIER}")
        logger.info(f"   ‚Ä¢ Prompt: max_tokens={cls.MAX_CONTEXT_TOKENS}, section={cls.INCLUDE_SECTION_IN_PROMPT}")
        logger.info(f"   ‚Ä¢ Response: format={cls.RESPONSE_FORMAT}, sources={cls.ALWAYS_SHOW_SOURCES}")
        logger.info(f"   ‚Ä¢ Telegram: enabled={cls.TELEGRAM_ENABLED}")
        logger.info(f"   ‚Ä¢ Device: force_cpu={cls.FORCE_CPU}")


def load_env_variable(var_name, default=None):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
    value = os.getenv(var_name, default)
    if value is None:
        raise EnvironmentError(f"Missing environment variable: {var_name}")
    return value


def make_request(url: str, auth_token: str, params: dict = None, method: str = 'GET') -> dict:
    """–î–µ–ª–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ Confluence API —Å retry-–ª–æ–≥–∏–∫–æ–π"""
    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json",
        "Authorization": f"Bearer {auth_token}"
    }
    max_retries = 3
    retry_delay = 2

    for attempt in range(max_retries):
        try:
            response = requests.request(
                method=method,
                url=url,
                params=params,
                headers=headers,
                timeout=30,
                verify=True
            )
            logger.debug(f"API [{response.status_code}]: {url.split('?')[0][-60:]}")

            if response.status_code == 401:
                raise ValueError("‚ùå 401: –ù–µ–≤–µ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω –∏–ª–∏ —Ñ–æ—Ä–º–∞—Ç –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏")
            elif response.status_code == 403:
                raise ValueError("‚ùå 403: –ù–µ—Ç –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞")
            elif response.status_code == 404:
                raise ValueError(f"‚ùå 404: –≠–Ω–¥–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {url}")
            elif response.status_code == 429:
                retry_after = int(response.headers.get('Retry-After', retry_delay * (attempt + 1)))
                logger.warning(f"‚ö†Ô∏è  Rate limit, –∂–¥—ë–º {retry_after} —Å–µ–∫...")
                time.sleep(retry_after)
                continue
            elif response.status_code >= 500:
                if attempt < max_retries - 1:
                    logger.warning(f"‚ö†Ô∏è  –°–µ—Ä–≤–µ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ {response.status_code}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 2}")
                    time.sleep(retry_delay * (attempt + 1))
                    continue
                raise ValueError(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ {response.status_code}")
            elif response.status_code >= 400:
                preview = response.text[:300].replace('\n', ' ')
                raise ValueError(f"‚ùå –û—à–∏–±–∫–∞ {response.status_code}: {preview}")

            if not response.text.strip():
                return {}
            return response.json()

        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                logger.warning(f"‚ö†Ô∏è  –¢–∞–π–º–∞—É—Ç, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 2}")
                time.sleep(retry_delay * (attempt + 1))
                continue
            raise ValueError(f"‚ùå –¢–∞–π–º–∞—É—Ç –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
        except requests.exceptions.ConnectionError as e:
            if attempt < max_retries - 1:
                logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 2}")
                time.sleep(retry_delay * (attempt + 1))
                continue
            raise ValueError(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}")
        except json.JSONDecodeError as e:
            preview = response.text[:400].replace('\n', ' ') if 'response' in locals() else "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞"
            raise ValueError(f"‚ùå –ù–µ JSON-–æ—Ç–≤–µ—Ç:\n{preview}\n–û—à–∏–±–∫–∞: {e}")

    raise ValueError("‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫")


def initialize_auth():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω –¥–ª—è Bearer-–∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    return load_env_variable("CONFLUENCE_API_KEY")


def html_to_text(html_data: str) -> str:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HTML –≤ —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
    if not html_data:
        return ""
    soup = BeautifulSoup(html_data, 'html.parser')
    for tag in soup(['script', 'style', 'nav', 'header', 'footer']):
        tag.decompose()
    for tag in soup.find_all(['br', 'p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):
        tag.append('\n')
    text = soup.get_text(separator=' ')
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    return ' '.join(lines)


def extract_metadata_from_confluence(page_data: dict, page_id: str, api_url: str) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Ç–≤–µ—Ç–∞ Confluence API."""
    if not isinstance(page_data, dict):
        logger.warning(f"‚ö†Ô∏è  extract_metadata_from_confluence: page_data –∏–º–µ–µ—Ç —Ç–∏–ø {type(page_data)}")
        return {
            'document_id': str(page_id),
            'title': '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è',
            'section': '',
            'chunk_index': 0,
            'total_chunks': 0,
            'url': f"{api_url}/pages/viewpage.action?pageId={page_id}",
            'page_version': '1',
            'last_updated': '',
            'space_key': '',
            'space_name': '',
            'content_type': 'page',
        }

    version_data = page_data.get('version', {})
    if not isinstance(version_data, dict):
        version_data = {}
    space_data = page_data.get('space', {})
    if not isinstance(space_data, dict):
        space_data = {}
    extensions_data = page_data.get('extensions', {})
    if not isinstance(extensions_data, dict):
        extensions_data = {}
    position_data = extensions_data.get('position', {})
    if not isinstance(position_data, dict):
        position_data = {}
    labels_data = page_data.get('labels', {})
    if not isinstance(labels_data, dict):
        labels_data = {}

    metadata = {
        "document_id": str(page_id),
        "title": page_data.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'),
        "section": position_data.get('position', ''),
        "chunk_index": 0,
        "total_chunks": 0,
        "url": f"{api_url}/pages/viewpage.action?pageId={page_id}",
        "page_version": str(version_data.get('number', 1)),
        "last_updated": version_data.get('when', ''),
        "space_key": space_data.get('key', ''),
        "space_name": space_data.get('name', ''),
        "content_type": "page",
    }

    labels_results = labels_data.get('results', [])
    if isinstance(labels_results, list) and labels_results:
        tags = [lbl.get('name', '') for lbl in labels_results if isinstance(lbl, dict) and lbl.get('name')]
        if tags:
            metadata['tags'] = tags

    return metadata


def singleton(cls):
    """–ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω—ã–π singleton"""
    instances = {}
    import threading
    lock = threading.Lock()

    def get_instance(*args, **kwargs):
        with lock:
            if cls not in instances:
                instances[cls] = cls(*args, **kwargs)
            return instances[cls]

    return get_instance


def get_redis_client():
    """–°–æ–∑–¥–∞—ë—Ç Redis-–∫–ª–∏–µ–Ω—Ç —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ env"""
    import redis
    return redis.Redis(
        host=load_env_variable("REDIS_HOST", "redis"),
        port=int(load_env_variable("REDIS_PORT", 6379)),
        db=int(load_env_variable("REDIS_DB", 0)),
        decode_responses=True,
        socket_connect_timeout=5,
        socket_timeout=5,
        retry_on_timeout=True,
        health_check_interval=30
    )


def format_datetime(dt: datetime) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç datetime –¥–ª—è Redis"""
    return dt.strftime('%Y-%m-%dT%H:%M:%S.%f%z')


def parse_datetime(dt_str: str) -> Optional[datetime]:
    """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É –≤—Ä–µ–º–µ–Ω–∏ –∏–∑ Redis"""
    if not dt_str:
        return None
    try:
        return datetime.strptime(dt_str, "%Y-%m-%dT%H:%M:%S.%f%z")
    except ValueError:
        try:
            return datetime.strptime(dt_str, "%Y-%m-%dT%H:%M:%S.%f+0000")
        except ValueError:
            logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É: {dt_str}")
            return None


def truncate_text(text: str, max_tokens: int, model_name: str = "gpt2") -> str:
    """–û–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ max_tokens —Å —É—á—ë—Ç–æ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏."""
    if len(text) <= max_tokens * 4:
        return text
    try:
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)
        tokens = tokenizer.encode(text, add_special_tokens=False)
        if len(tokens) <= max_tokens:
            return text
        truncated = tokenizer.decode(tokens[:max_tokens])
        return truncated + "..."
    except Exception:
        return text[:max_tokens * 4] + "..."


def format_markdown_response(text: str, sources: List[Dict[str, str]] = None) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –≤ Markdown —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏."""
    if not sources or not Config.ALWAYS_SHOW_SOURCES:
        return text

    source_lines = []
    for src in sources[:Config.MAX_SOURCE_LINKS]:
        title = src.get('title', '–î–æ–∫—É–º–µ–Ω—Ç')
        url = src.get('url', '#')
        section = src.get('section', '')
        if section and Config.INCLUDE_SECTION_IN_PROMPT:
            display_title = f"{title} ‚Äî {section}"
        else:
            display_title = title
        source_lines.append(f"‚Ä¢ [{display_title}]({url})")

    if source_lines:
        return f"{text}\n\nüìé **–ò—Å—Ç–æ—á–Ω–∏–∫–∏**:\n" + "\n".join(source_lines)
    return text
