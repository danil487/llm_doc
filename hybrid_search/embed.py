# hybrid_search/embed.py
from sentence_transformers import SentenceTransformer, CrossEncoder
from rank_bm25 import BM25Okapi
from hybrid_search.utils import singleton, logger, Config
import re
import os


@singleton
class Embed:
    def __init__(self):
        # ‚úÖ –û–ü–†–ï–î–ï–õ–Ø–ï–ú —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        self.device = self._get_device()
        logger.info(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        # Dense embedding –º–æ–¥–µ–ª—å
        logger.info("üîß –ó–∞–≥—Ä—É–∑–∫–∞ embedding –º–æ–¥–µ–ª–∏...")
        self.dense_model = SentenceTransformer(
            "sentence-transformers/all-mpnet-base-v2",
            device=self.device  # ‚Üê –ò–°–ü–†–ê–í–õ–ï–ù–û
        )

        # Reranker (cross-encoder) –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        logger.info(f"üîß –ó–∞–≥—Ä—É–∑–∫–∞ reranker –º–æ–¥–µ–ª–∏: {Config.RERANKER_MODEL}")
        self.reranker = CrossEncoder(
            Config.RERANKER_MODEL,
            device=self.device  # ‚Üê –ò–°–ü–†–ê–í–õ–ï–ù–û
        )

        # Sparse: BM25
        self.bm25 = None
        self.corpus_tokens = []
        self._bm25_initialized = False
        logger.info("‚úÖ Embed + Reranker –≥–æ—Ç–æ–≤—ã")

    def _get_device(self) -> str:
        """‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
        force_cpu = os.getenv("FORCE_CPU", "false").lower() == "true"
        if force_cpu:
            logger.info("‚ö†Ô∏è  –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU (FORCE_CPU=true)")
            return "cpu"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º CUDA
        try:
            import torch
            if torch.cuda.is_available():
                device = "cuda"
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
                logger.info(f"‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {gpu_name} ({gpu_memory:.1f} GB)")
                return device
            else:
                logger.warning("‚ö†Ô∏è  CUDA –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
                return "cpu"
        except ImportError:
            logger.warning("‚ö†Ô∏è  PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
            return "cpu"
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ CUDA: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
            return "cpu"

    def _tokenize(self, text: str) -> list[str]:
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è BM25"""
        return re.findall(r'\b[a-z–∞-—è—ë0-9]{2,}\b', text.lower())

    def embed_text(self, text: str) -> list[float]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dense-–≤–µ–∫—Ç–æ—Ä (768-dim)"""
        dense_embeddings = self.dense_model.encode(
            text,
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        dense_vector = dense_embeddings.tolist()
        if isinstance(dense_vector[0], list):
            dense_vector = dense_vector[0]
        return dense_vector

    def embed_sparse(self, text: str) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç sparse-–≤–µ–∫—Ç–æ—Ä –¥–ª—è BM25"""
        tokens = self._tokenize(text)
        if self._bm25_initialized and self.bm25 and tokens:
            scores = self.bm25.get_scores(tokens)
            indices = [i for i, s in enumerate(scores) if s > 1e-6]
            values = [float(scores[i]) for i in indices]
        else:
            indices, values = [0], [1e-9]
        return {"indices": indices, "values": values}

    def rerank(self, query: str, chunks: list[dict]) -> list[dict]:
        """–†–∞–Ω–∂–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ —Å –ø–æ–º–æ—â—å—é cross-encoder"""
        if not chunks:
            return []

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä—ã (query, chunk_text) –¥–ª—è reranker
        pairs = [[query, chunk.get('text', chunk.get('content', ''))] for chunk in chunks]

        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º scores (0.0 - 1.0)
        try:
            scores = self.reranker.predict(pairs)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ rerank: {e}")
            # Fallback: —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–º—É score
            return sorted(chunks, key=lambda x: x.get('score', 0), reverse=True)

        # –î–æ–±–∞–≤–ª—è–µ–º rerank_score –∫ —á–∞–Ω–∫–∞–º
        for chunk, score in zip(chunks, scores):
            chunk['rerank_score'] = float(score)

        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ä–æ–≥—É –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
        filtered = [c for c in chunks if c.get('rerank_score', 0) >= Config.RERANK_MIN_SCORE]
        sorted_chunks = sorted(filtered, key=lambda x: x.get('rerank_score', 0), reverse=True)

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-K
        return sorted_chunks[:Config.RERANK_TOP_K]

    def fit_bm25(self, documents: list[str]):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BM25 –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if not documents:
            logger.warning("‚ö†Ô∏è  –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ BM25")
            return

        logger.info(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BM25 –Ω–∞ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö...")
        corpus_tokens = [self._tokenize(doc) for doc in documents if doc and doc.strip()]
        corpus_tokens = [t for t in corpus_tokens if t]

        if corpus_tokens:
            self.bm25 = BM25Okapi(corpus_tokens)
            self.corpus_tokens = corpus_tokens
            self._bm25_initialized = True
            total_tokens = sum(len(t) for t in corpus_tokens)
            logger.info(f"‚úÖ BM25 –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {len(corpus_tokens)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {total_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
        else:
            logger.warning("‚ö†Ô∏è  BM25 –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def embed_texts_batch(self, texts: list[str]) -> list[list[float]]:
        """–ü–∞–∫–µ—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–±—ã—Å—Ç—Ä–µ–µ –≤ 5-10 —Ä–∞–∑)"""
        if not texts:
            return []

        dense_embeddings = self.dense_model.encode(
            texts,
            convert_to_numpy=True,
            normalize_embeddings=True,
            batch_size=32,
            show_progress_bar=False
        )

        if len(dense_embeddings.shape) == 1:
            dense_embeddings = dense_embeddings.reshape(1, -1)
        return dense_embeddings.tolist()

    def embed_sparse_batch(self, texts: list[str]) -> list[dict]:
        """–ü–∞–∫–µ—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è sparse-–≤–µ–∫—Ç–æ—Ä–æ–≤"""
        results = []
        for text in texts:
            results.append(self.embed_sparse(text))
        return results
